{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d51384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install beautiful soup if needed\n",
    "'''pip install requests beautifulsoup4 pandas'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ff5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE DATA\n",
    "\n",
    "# Specify your CSV file\n",
    "csv_file = '2024_turkey_car_market.csv'\n",
    "\n",
    "# Read the CSV file if it exists\n",
    "try:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Remove any rows with all NaN values\n",
    "    df.dropna(how='all', inplace=True)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Ensure that 'Page' column is numeric, replacing any non-numeric values with NaN\n",
    "if 'Page' not in df.columns:\n",
    "    df['Page'] = np.nan  # Ensure the 'Page' column exists\n",
    "\n",
    "df['Page'] = pd.to_numeric(df['Page'], errors='coerce')\n",
    "\n",
    "# Get the last scraped page number\n",
    "last_scraped_page = df['Page'].max()\n",
    "if pd.isna(last_scraped_page):\n",
    "    last_scraped_page = 0\n",
    "else:\n",
    "    last_scraped_page = int(last_scraped_page)\n",
    "\n",
    "print(f\"Last scraped page: {last_scraped_page}\")\n",
    "\n",
    "# Define your base URL\n",
    "base_url = 'https://www.arabam.com'\n",
    "\n",
    "# List of search URLs to scrape\n",
    "search_urls = [\n",
    "    'https://www.arabam.com/ikinci-el?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/alfa-romeo?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/audi?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/bentley',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/bmw-1-serisi?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/bmw-3-serisi?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/bmw-5-serisi?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/chevrolet?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/citroen?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/dacia?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/ferrari',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/fiat?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/fiat-egea?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/fiat-linea?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/fiat-palio?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/fiat-punto?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/ford?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/ford-fiesta?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/ford-focus?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/ford-mondeo?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/ford-mustang?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/honda?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/honda-civic?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/hyundai?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/hyundai-accent?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/hyundai-accent-blue?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/hyundai-i20?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/jaguar',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/kia?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/lada',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/lamborghini',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/maserati',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/mazda',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/mercedes-benz?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/mini',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/mitsubishi',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/nissan',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/opel?page=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/opel-astra?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/opel-corsa?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/opel-vectra?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/peugeot?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/peugeot-206?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/peugeot-307?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/porsche',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/renault?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/renault-clio?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/renault-fluence?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/renault-laguna?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/renault-megane?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/renault-symbol?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/rolls-royce',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/rover',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/seat?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/skoda?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/subaru',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/suzuki',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/tata',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/tofas?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/toyota?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/toyota-corolla?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/volkswagen?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/volkswagen-golf?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/volkswagen-jetta?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/volkswagen-passat?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/volkswagen-polo?take=50',\n",
    "    'https://www.arabam.com/ikinci-el/otomobil/volvo?take=50'    \n",
    "]\n",
    "\n",
    "# Setup retry strategy for requests\n",
    "retry_strategy = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", adapter)\n",
    "http.mount(\"http://\", adapter)\n",
    "\n",
    "# Define Function to Extract Links from JavaScript\n",
    "def extract_links_from_js(script_text):\n",
    "    urls = re.findall(r'\"url\":\\s*window\\.location\\.origin\\s*\\+\\s*\"([^\"]+)\"', script_text)\n",
    "    return [base_url + url for url in urls]\n",
    "\n",
    "                         \n",
    "# Define Function to Scrape Search Page\n",
    "def scrape_search_page(page_number, search_url):\n",
    "    page_url = f\"{search_url}&page={page_number}\"  # Append page number to the search URL\n",
    "    try:\n",
    "        response = http.get(page_url)\n",
    "        response.raise_for_status()  # Raise an exception for any HTTP error\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        script_tag = soup.find('script', string=re.compile(r'\"url\":\\s*window\\.location\\.origin'))\n",
    "        if script_tag:\n",
    "            car_links = extract_links_from_js(script_tag.string)\n",
    "        else:\n",
    "            car_links = []\n",
    "        \n",
    "        if not car_links:\n",
    "            print(f\"Warning: No car links found on page {page_url}\")\n",
    "        return car_links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page {page_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_car_details(car_link):\n",
    "    try:\n",
    "        response = http.get(car_link)\n",
    "        response.raise_for_status()  # Raise an exception for any HTTP error\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        def find_text_by_label(label):\n",
    "            property_item_divs = soup.find_all('div', class_='property-item')\n",
    "            for div in property_item_divs:\n",
    "                key = div.find('div', class_='property-key')\n",
    "                if key and key.get_text(strip=True) == label:\n",
    "                    value = div.find('div', class_='property-value')\n",
    "                    if value:\n",
    "                        return value.get_text(strip=True)\n",
    "            print(f\"Warning: Label '{label}' not found on page {car_link}\")\n",
    "            return None\n",
    "\n",
    "        def find_price():\n",
    "            price_element = soup.find('div', class_='product-price')\n",
    "            if price_element:\n",
    "                return price_element.get_text(strip=True)\n",
    "            else:\n",
    "                print(f\"Warning: Price not found on page {car_link}\")\n",
    "            return None\n",
    "\n",
    "        # Check if ad number already exists, if not, add it to the set and proceed\n",
    "        ad_number = find_text_by_label('İlan No')\n",
    "        if ad_number in scraped_ad_numbers:\n",
    "            print(f\"Ad number {ad_number} already exists, skipping...\")\n",
    "            return None\n",
    "        scraped_ad_numbers.add(ad_number)\n",
    "\n",
    "        data = {\n",
    "            \"Link\": car_link,  # Add Link column with the car link\n",
    "            \"İlan No\": ad_number,\n",
    "            \"İlan Tarihi\": find_text_by_label('İlan Tarihi'),\n",
    "            \"Marka\": find_text_by_label('Marka'),\n",
    "            \"Seri\": find_text_by_label('Seri'),\n",
    "            \"Model\": find_text_by_label('Model'),\n",
    "            \"Yıl\": find_text_by_label('Yıl'),\n",
    "            \"Kilometre\": find_text_by_label('Kilometre'),\n",
    "            \"Vites Tipi\": find_text_by_label('Vites Tipi'),\n",
    "            \"Yakıt Tipi\": find_text_by_label('Yakıt Tipi'),\n",
    "            \"Kasa Tipi\": find_text_by_label('Kasa Tipi'),\n",
    "            \"Renk\": find_text_by_label('Renk'),\n",
    "            \"Motor Hacmi\": find_text_by_label('Motor Hacmi'),\n",
    "            \"Motor Gücü\": find_text_by_label('Motor Gücü'),\n",
    "            \"Çekiş\": find_text_by_label('Çekiş'),\n",
    "            \"Araç Durumu\": find_text_by_label('Araç Durumu'),\n",
    "            \"Ort. Yakıt Tüketimi\": find_text_by_label('Ort. Yakıt Tüketimi'),\n",
    "            \"Yakıt Deposu\": find_text_by_label('Yakıt Deposu'),\n",
    "            \"Boya-değişen\": find_text_by_label('Boya-değişen'),\n",
    "            \"Takasa Uygun\": find_text_by_label('Takasa Uygun'),\n",
    "            \"Kimden\": find_text_by_label('Kimden'),\n",
    "            \"Fiyat\": find_price(),\n",
    "            \"Page\": current_page  # Add the current page number to the data\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping car details from {car_link}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_all_cars(start_page, end_page):\n",
    "    all_car_data = []\n",
    "    global current_page\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        current_page = page_number\n",
    "        for search_url in search_urls:\n",
    "            print(f\"Scraping page {page_number} for search URL: {search_url}...\")\n",
    "            car_links = scrape_search_page(page_number, search_url)\n",
    "            for car_link in car_links:\n",
    "                try:\n",
    "                    car_data = scrape_car_details(car_link)\n",
    "                    if car_data:\n",
    "                        all_car_data.append(car_data)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Request failed for {car_link}: {e}\")\n",
    "                    continue  # Skip this link and move to the next one\n",
    "            time.sleep(3)  # Add a delay to avoid overwhelming the server\n",
    "        if all_car_data:\n",
    "            # Save the intermediate data after each page\n",
    "            df_new = pd.DataFrame(all_car_data)\n",
    "            global df\n",
    "            df = pd.concat([df, df_new], ignore_index=True)\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            all_car_data.clear()  # Clear the data after saving\n",
    "    return all_car_data\n",
    "\n",
    "# Scrape car data from this many pages, resuming from the last scraped page\n",
    "current_page = last_scraped_page + 1\n",
    "end_page = 50\n",
    "\n",
    "car_data = scrape_all_cars(start_page=current_page, end_page=end_page)\n",
    "\n",
    "# Convert the data to a DataFrame and save to a CSV file if there is any remaining data\n",
    "if car_data:\n",
    "    df_new = pd.DataFrame(car_data)\n",
    "    df = pd.concat([df, df_new], ignore_index=True)  # Append new data to the existing DataFrame\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "print('Scraping completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# If the above scraping fails, use this to start fresh\n",
    "import pandas as pd\n",
    "\n",
    "# Specify your CSV file\n",
    "csv_file = 'car_data.csv'\n",
    "\n",
    "# Create an empty DataFrame with the columns you intend to use\n",
    "empty_df = pd.DataFrame(columns=[\n",
    "    'İlan Tarihi', 'Marka', 'Seri', 'Model', 'Yıl', 'Kilometre', 'Vites Tipi', \n",
    "    'Yakıt Tipi', 'Kasa Tipi', 'Renk', 'Motor Hacmi', 'Motor Gücü', 'Araç Durumu', \n",
    "    'Ort. Yakıt Tüketimi', 'Yakıt Deposu', 'Boya-değişen', 'Takasa Uygun', \n",
    "    'Kimden', 'Fiyat', 'Link', 'İlan No', 'Page'\n",
    "])\n",
    "\n",
    "# Save the empty DataFrame to the CSV file\n",
    "empty_df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"The file '{csv_file}' has been reset to an empty state.\")\n",
    "\n",
    "# empty the set scraped_ad_numbers\n",
    "scraped_ad_numbers.clear()\n",
    "last_scraped_page = 0\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
